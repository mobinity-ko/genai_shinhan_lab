# %% 0. íŒŒì¼ í—¤ë” ë° ì„¤ëª…
"""
POTENS APIë¥¼ LangChainì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ Custom Wrapper

ì‚¬ìš©ë²•:
1. .envì— POTENS_API_KEY ì„¤ì •
2. PotensLLM ë˜ëŠ” PotensChatModel ì‚¬ìš©
3. LangChainì˜ ëª¨ë“  ê¸°ëŠ¥ (Chain, Agent ë“±)ê³¼ í˜¸í™˜

ì˜ˆì‹œ ì‹¤í–‰:
    python potens_wrapper.py  # ì˜ˆì‹œ ì½”ë“œ ì‹¤í–‰
    
ëª¨ë“ˆë¡œ ì‚¬ìš©:
    from potens_wrapper import PotensLLM, PotensChatModel
"""

import os
import requests
from typing import Any, List, Optional, Dict
from dotenv import load_dotenv

from langchain_core.language_models.llms import LLM
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.outputs import ChatResult, ChatGeneration
from langchain_core.callbacks import CallbackManagerForLLMRun

# %% 1. ê¸°ë³¸ LLM Wrapper (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì…ì¶œë ¥)

class PotensLLM(LLM):
    """
    POTENS APIë¥¼ LangChain LLMìœ¼ë¡œ ë˜í•‘
    
    ìš©ë„: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±, Chainì—ì„œ ì‚¬ìš©
    """
    
    api_key: str = None
    api_url: str = "https://ai.potens.ai/api/chat"
    temperature: float = 0.7
    max_tokens: int = 2000
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if not self.api_key:
            load_dotenv()
            self.api_key = os.getenv("POTENS_API_KEY")
            if not self.api_key:
                raise ValueError("POTENS_API_KEYë¥¼ .env íŒŒì¼ì— ì„¤ì •í•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬í•˜ì„¸ìš”.")
    
    @property
    def _llm_type(self) -> str:
        """LLM íƒ€ì… ì‹ë³„ì"""
        return "potens"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """
        ì‹¤ì œ API í˜¸ì¶œ ë¡œì§
        
        Args:
            prompt: ì‚¬ìš©ì ì…ë ¥
            stop: ìƒì„± ì¤‘ì§€ í† í° (í˜„ì¬ ë¯¸ì§€ì›)
            run_manager: LangChain ì½œë°± ë§¤ë‹ˆì €
        
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        body = {"prompt": prompt}
        
        # kwargsì—ì„œ system_prompt ì¶”ì¶œ
        if "system_prompt" in kwargs:
            body["system_prompt"] = kwargs["system_prompt"]
        
        try:
            response = requests.post(
                self.api_url,
                headers=headers,
                json=body,
                timeout=60
            )
            response.raise_for_status()
            
            api_response = response.json()
            return api_response.get('message', 'Error: No message in response')
            
        except requests.RequestException as e:
            return f"API Error: {str(e)}"


# %% 2. ChatModel Wrapper (ëŒ€í™”í˜•, Agent ì§€ì›)

class PotensChatModel(BaseChatModel):
    """
    POTENS APIë¥¼ LangChain ChatModelë¡œ ë˜í•‘
    
    ìš©ë„: 
    - ë©€í‹°í„´ ëŒ€í™”
    - Agent êµ¬ì¶• (ReAct íŒ¨í„´)
    - ëŒ€í™” ì´ë ¥ ê´€ë¦¬
    """
    
    api_key: str = None
    api_url: str = "https://ai.potens.ai/api/chat"
    temperature: float = 0.7
    max_tokens: int = 2000
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if not self.api_key:
            load_dotenv()
            self.api_key = os.getenv("POTENS_API_KEY")
            if not self.api_key:
                raise ValueError("POTENS_API_KEYë¥¼ .env íŒŒì¼ì— ì„¤ì •í•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬í•˜ì„¸ìš”.")
    
    @property
    def _llm_type(self) -> str:
        return "potens_chat"
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ ì‘ë‹µ ìƒì„±
        
        Args:
            messages: [SystemMessage, HumanMessage, AIMessage, ...]
        
        Returns:
            ChatResult with AIMessage
        """
        # ë©”ì‹œì§€ë¥¼ POTENS API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        prompt, system_prompt = self._messages_to_prompt(messages)
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        body = {"prompt": prompt}
        if system_prompt:
            body["system_prompt"] = system_prompt
        
        try:
            response = requests.post(
                self.api_url,
                headers=headers,
                json=body,
                timeout=60
            )
            response.raise_for_status()
            
            api_response = response.json()
            content = api_response.get('message', 'Error: No message in response')
            
            # ChatGeneration ê°ì²´ ìƒì„±
            message = AIMessage(content=content)
            generation = ChatGeneration(message=message)
            
            return ChatResult(generations=[generation])
            
        except requests.RequestException as e:
            error_message = AIMessage(content=f"API Error: {str(e)}")
            generation = ChatGeneration(message=error_message)
            return ChatResult(generations=[generation])
    
    def _messages_to_prompt(self, messages: List[BaseMessage]) -> tuple[str, Optional[str]]:
        """
        LangChain ë©”ì‹œì§€ë¥¼ POTENS API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Returns:
            (prompt, system_prompt)
        """
        system_prompt = None
        conversation = []
        
        for msg in messages:
            if isinstance(msg, SystemMessage):
                system_prompt = msg.content
            elif isinstance(msg, HumanMessage):
                conversation.append(f"ì‚¬ìš©ì: {msg.content}")
            elif isinstance(msg, AIMessage):
                conversation.append(f"AI: {msg.content}")
        
        # ëŒ€í™” ì´ë ¥ì„ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ê²°í•©
        prompt = "\n".join(conversation)
        
        return prompt, system_prompt


# ============================================================================
# ì´ ì•„ë˜ëŠ” ì§ì ‘ ì‹¤í–‰í•  ë•Œë§Œ ë™ì‘í•©ë‹ˆë‹¤ (import ì‹œì—ëŠ” ì‹¤í–‰ ì•ˆ ë¨)
# ============================================================================

if __name__ == "__main__":
    # %% 3. ì‚¬ìš© ì˜ˆì‹œ 1: ê¸°ë³¸ LLM ì‚¬ìš©
    
    print("="*80)
    print("ğŸ“ ì˜ˆì‹œ 1: ê¸°ë³¸ LLM ì‚¬ìš© (ë‹¨ì¼ í˜¸ì¶œ)")
    print("="*80)
    
    llm = PotensLLM()
    
    # ê°„ë‹¨í•œ ì§ˆë¬¸
    response = llm.invoke("Pythonìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²• 3ê°€ì§€ ì•Œë ¤ì¤˜")
    print(f"\nğŸ¤– ì‘ë‹µ:\n{response}")
    
    # %% 4. ì‚¬ìš© ì˜ˆì‹œ 2: Chainê³¼ í•¨ê»˜ ì‚¬ìš© (LCEL ë°©ì‹)
    
    from langchain_core.prompts import PromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    
    print("\n" + "="*80)
    print("â›“ï¸ ì˜ˆì‹œ 2: LCEL Chain ì‚¬ìš© (í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿)")
    print("="*80)
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    prompt = PromptTemplate(
        input_variables=["language", "task"],
        template="""
ë‹¹ì‹ ì€ í”„ë¡œê·¸ë˜ë° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì–¸ì–´: {language}
ì‘ì—…: {task}

ìœ„ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ê³ , ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
    )
    
    # LCEL ë°©ì‹: | ì—°ì‚°ìë¡œ Chain êµ¬ì„±
    chain = prompt | llm | StrOutputParser()
    
    # ì‹¤í–‰
    result = chain.invoke({
        "language": "Python",
        "task": "CSV íŒŒì¼ì„ ì½ì–´ì„œ ê²°ì¸¡ì¹˜ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ì±„ìš°ê¸°"
    })
    
    print(f"\nğŸ¤– ì‘ë‹µ:\n{result}")
    
    # %% 5. ì‚¬ìš© ì˜ˆì‹œ 3: ChatModelë¡œ ë©€í‹°í„´ ëŒ€í™”
    
    print("\n" + "="*80)
    print("ğŸ’¬ ì˜ˆì‹œ 3: ChatModel - ë©€í‹°í„´ ëŒ€í™”")
    print("="*80)
    
    chat_model = PotensChatModel()
    
    # ëŒ€í™” ì‹œì‘
    messages = [
        SystemMessage(content="ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
        HumanMessage(content="Pandasì—ì„œ groupbyë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜")
    ]
    
    response1 = chat_model.invoke(messages)
    print(f"\nğŸ¤– ì‘ë‹µ 1:\n{response1.content}")
    
    # ëŒ€í™” ì´ë ¥ì— ì¶”ê°€
    messages.append(response1)
    messages.append(HumanMessage(content="ê·¸ëŸ¼ ì—¬ëŸ¬ ì»¬ëŸ¼ìœ¼ë¡œ groupbyí•˜ë ¤ë©´?"))
    
    response2 = chat_model.invoke(messages)
    print(f"\nğŸ¤– ì‘ë‹µ 2:\n{response2.content}")
    
    # %% 6. ì‚¬ìš© ì˜ˆì‹œ 4: Sequential Chain (ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸)
    
    from langchain_core.runnables import RunnablePassthrough
    
    print("\n" + "="*80)
    print("ğŸ”„ ì˜ˆì‹œ 4: LCEL Sequential Chain - ë¶„ì„ íŒŒì´í”„ë¼ì¸")
    print("="*80)
    
    # Chain 1: ë°ì´í„° ì´í•´
    understand_prompt = PromptTemplate(
        input_variables=["data_info"],
        template="ë‹¤ìŒ ë°ì´í„°ì˜ íŠ¹ì§•ì„ 3ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n{data_info}"
    )
    
    # Chain 2: ë¶„ì„ ê³„íš
    plan_prompt = PromptTemplate(
        input_variables=["summary"],
        template="""
ë°ì´í„° ìš”ì•½: {summary}

ì´ ë°ì´í„°ë¡œ í•  ìˆ˜ ìˆëŠ” ìœ ì˜ë¯¸í•œ ë¶„ì„ 3ê°€ì§€ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.
ê° ë¶„ì„ë§ˆë‹¤ ì‚¬ìš©í•  Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ëª…ì‹œí•´ì£¼ì„¸ìš”.
"""
    )
    
    # LCEL ë°©ì‹ìœ¼ë¡œ Sequential Chain êµ¬ì„±
    # Step 1: ë°ì´í„° ìš”ì•½
    chain_step1 = understand_prompt | llm | StrOutputParser()
    
    # Step 2: ìš”ì•½ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ë¶„ì„ ê³„íš ìƒì„±
    chain_step2 = (
        {"summary": chain_step1}  # step1ì˜ ì¶œë ¥ì„ summaryë¡œ ì „ë‹¬
        | plan_prompt 
        | llm 
        | StrOutputParser()
    )
    
    # ì‹¤í–‰
    sample_data = """
ì»¬ëŸ¼: user_id, age, gender, purchase_amount, purchase_date
í–‰ ìˆ˜: 10,000
ê²°ì¸¡ì¹˜: age 5%, gender 2%
ì´ìƒì¹˜: purchase_amountì— ê·¹ë‹¨ê°’ ì¡´ì¬
"""
    
    print("\nğŸ“Š ì‹¤í–‰ ì¤‘...")
    summary = chain_step1.invoke({"data_info": sample_data})
    print(f"\nğŸ“Š ë°ì´í„° ìš”ì•½:\n{summary}")
    
    analysis_plan = chain_step2.invoke({"data_info": sample_data})
    print(f"\nğŸ“‹ ë¶„ì„ ê³„íš:\n{analysis_plan}")
    
    # %% 7. ì‚¬ìš© ì˜ˆì‹œ 5: Pseudo-Agent (ReAct íŒ¨í„´)
    
    print("\n" + "="*80)
    print("ğŸ¤– ì˜ˆì‹œ 5: Pseudo-Agent - ReAct íŒ¨í„´")
    print("="*80)
    
    class SimplePseudoAgent:
        """
        ReAct íŒ¨í„´ì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ Pseudo-Agent
        
        Function Calling ì—†ì´ë„ ë™ì‘í•˜ëŠ” Agent
        """
        
        def __init__(self, llm: PotensLLM):
            self.llm = llm
            self.history = []
        
        def run(self, question: str, max_iterations: int = 3):
            """
            ì§ˆë¬¸ì— ëŒ€í•´ ReAct íŒ¨í„´ìœ¼ë¡œ ë‹µë³€
            
            Args:
                question: ì‚¬ìš©ì ì§ˆë¬¸
                max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
            """
            system_prompt = """
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ Agentì…ë‹ˆë‹¤. ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:

Thought: (ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ìƒê°)
Action: python_repl
Action Input: (ì‹¤í–‰í•  Python ì½”ë“œ)

ì‚¬ìš©ìê°€ "Observation: [ê²°ê³¼]"ë¥¼ ì œê³µí•˜ë©´, ê·¸ ê²°ê³¼ë¥¼ ë¶„ì„í•´ì„œ ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•˜ì„¸ìš”.
ìµœì¢… ë‹µë³€ì„ ì œê³µí•  ì¤€ë¹„ê°€ ë˜ë©´ "Final Answer: [ë‹µë³€]" í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.
"""
            
            current_prompt = f"ì‚¬ìš©ì ì§ˆë¬¸: {question}\n\në‹µë³€ì„ ì‹œì‘í•˜ì„¸ìš”."
            
            for i in range(max_iterations):
                print(f"\n{'â”€'*60}")
                print(f"ğŸ”„ ë°˜ë³µ {i+1}/{max_iterations}")
                print(f"{'â”€'*60}")
                
                # LLMì—ê²Œ ë‹¤ìŒ í–‰ë™ ë¬¼ì–´ë³´ê¸°
                response = self.llm.invoke(
                    f"{system_prompt}\n\n{current_prompt}",
                )
                
                print(f"\nğŸ¤– LLM ì‘ë‹µ:\n{response}")
                
                # Final Answer í™•ì¸
                if "Final Answer:" in response:
                    final_answer = response.split("Final Answer:")[1].strip()
                    print(f"\nâœ… ìµœì¢… ë‹µë³€:\n{final_answer}")
                    return final_answer
                
                # Action Input ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ì •ê·œì‹ ë“±ìœ¼ë¡œ íŒŒì‹±)
                if "Action Input:" in response:
                    print("\nğŸ’¡ ì‚¬ìš©ì: (ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì „ë‹¬)")
                    print("   ì˜ˆì‹œ: Observation: [ì½”ë“œ ì‹¤í–‰ ê²°ê³¼]")
                    
                    # ì‹œë®¬ë ˆì´ì…˜: ì‚¬ìš©ìê°€ ê²°ê³¼ë¥¼ ì „ë‹¬í–ˆë‹¤ê³  ê°€ì •
                    observation = "ë°ì´í„°í”„ë ˆì„ì˜ í‰ê· ê°’ì€ 42ì…ë‹ˆë‹¤."
                    current_prompt += f"\n\n{response}\n\nObservation: {observation}\n\në‹¤ìŒ í–‰ë™ì„ ê²°ì •í•˜ì„¸ìš”."
                else:
                    current_prompt += f"\n\n{response}"
            
            return "ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬"
    
    # Agent ì‹¤í–‰
    agent = SimplePseudoAgent(llm=llm)
    result = agent.run("ë°ì´í„°ì˜ í‰ê· ê°’ì„ êµ¬í•˜ê³ , ê·¸ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ë¥¼ ì„¤ëª…í•´ì¤˜")
    
    # %% 8. êµìœ¡ìš© í…œí”Œë¦¿: Streamlit + Agent ì—°ë™ ê¸°ë³¸ êµ¬ì¡°
    
    print("\n" + "="*80)
    print("ğŸ“± ì˜ˆì‹œ 6: Streamlit ì—°ë™ ê¸°ë³¸ êµ¬ì¡° (ì½”ë“œ í…œí”Œë¦¿)")
    print("="*80)
    
    streamlit_template = '''
"""
Streamlit + POTENS Agent ê¸°ë³¸ í…œí”Œë¦¿
ì‹¤í–‰: streamlit run app.py
"""

import streamlit as st
from potens_wrapper import PotensChatModel
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë‚˜ë§Œì˜ AI ë¶„ì„ ë„êµ¬", page_icon="ğŸ¤–")
st.title("ğŸ¤– AI ë°ì´í„° ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸")

# LLM ì´ˆê¸°í™”
@st.cache_resource
def get_llm():
    return PotensChatModel()

llm = get_llm()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ëŒ€í™” ì´ë ¥ ì €ì¥)
if "messages" not in st.session_state:
    st.session_state.messages = [
        SystemMessage(content="ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.")
    ]

# ì´ì „ ëŒ€í™” í‘œì‹œ
for msg in st.session_state.messages[1:]:  # SystemMessage ì œì™¸
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.write(msg.content)
    elif isinstance(msg, AIMessage):
        with st.chat_message("assistant"):
            st.write(msg.content)

# ì‚¬ìš©ì ì…ë ¥
if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.write(user_input)
    
    # ë©”ì‹œì§€ ì´ë ¥ì— ì¶”ê°€
    st.session_state.messages.append(HumanMessage(content=user_input))
    
    # LLM ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ìƒê° ì¤‘..."):
            response = llm.invoke(st.session_state.messages)
            st.write(response.content)
    
    # ì‘ë‹µë„ ì´ë ¥ì— ì¶”ê°€
    st.session_state.messages.append(response)

# ì‚¬ì´ë“œë°”: ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
with st.sidebar:
    if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.")
        ]
        st.rerun()
'''
    
    print(streamlit_template)
    
    # %% 9. ë‹¤ìŒ ë‹¨ê³„ ê°€ì´ë“œ
    
    print("\n" + "="*80)
    print("ğŸš€ ë‹¤ìŒ ë‹¨ê³„: êµìœ¡ ê³¼ì • êµ¬í˜„")
    print("="*80)
    
    print("""
âœ… ì™„ë£Œëœ ê²ƒ:
  - POTENS API LangChain Wrapper
  - ê¸°ë³¸ LLM, ChatModel êµ¬í˜„
  - Chain, Agent íŒ¨í„´ ì˜ˆì‹œ

ğŸ“š êµìœ¡ ê³¼ì • êµ¬í˜„ ìˆœì„œ:

1. 2ë¶€: LangChain Chain Deep Dive
   âœ“ Sequential Chainìœ¼ë¡œ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸
   âœ“ ì´ íŒŒì¼ì˜ ì˜ˆì‹œ 4 í™œìš©

2. 4ë¶€: Streamlit + Agent ì—°ë™
   âœ“ ì´ íŒŒì¼ì˜ ì˜ˆì‹œ 6 í…œí”Œë¦¿ ì‚¬ìš©
   âœ“ CSV ì—…ë¡œë“œ + ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€

3. 5ë¶€: ììœ¨ì  EDA Agent
   âœ“ Pseudo-Agent íŒ¨í„´ (ì˜ˆì‹œ 5 ê¸°ë°˜)
   âœ“ ReAct í”„ë¡¬í”„íŒ… + ìˆ˜ë™ Tool ì‹¤í–‰

ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:
  - Function Calling ì—†ì´ë„ ReAct íŒ¨í„´ìœ¼ë¡œ Agent êµ¬í˜„ ê°€ëŠ¥
  - ëŒ€í™” ì´ë ¥ì€ messages ë°°ì—´ë¡œ ê´€ë¦¬
  - Streamlit session_stateë¡œ ì—°ì†ì„± í™•ë³´
  
ğŸ”§ ê°œì„  ê°€ëŠ¥ ì‚¬í•­:
  - Tool ì‹¤í–‰ì„ ìë™í™” (safe_exec êµ¬í˜„)
  - ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
  - í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
""")